{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9dc66b73",
   "metadata": {},
   "source": [
    "# Loan Text Analysis "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "484d026c",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "7922862e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import random\n",
    "\n",
    "# Modeling\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import LinearSVC, SVC\n",
    "from sklearn.metrics import classification_report, confusion_matrix, precision_score, recall_score, f1_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "\n",
    "# HuggingFace transformers\n",
    "from transformers import BertTokenizerFast, BertForSequenceClassification, Trainer, TrainingArguments\n",
    "from datasets import Dataset\n",
    "import torch\n",
    "\n",
    "pd.set_option('display.max_colwidth', 300)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a91d1ea",
   "metadata": {},
   "source": [
    "## 2: Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "495dd938",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reddit CSV\n",
    "reddit_df = pd.read_csv(\"../data/processed/large_sample_reddit_predatory_loan_posts_cleaned.csv\")\n",
    "reddit_df = reddit_df[['post_text', 'label']]\n",
    "reddit_df.columns = ['text', 'label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "42f42278",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scraped Fair Loans (Older data)\n",
    "webpages_df = pd.read_csv(\"../data/processed/fair_loan_texts.csv\")\n",
    "pdf_df = pd.read_csv(\"../data/processed/fair_loan_pdfs.csv\")\n",
    "webpages_df['label'] = 'non_predatory'\n",
    "pdf_df['label'] = 'non_predatory'\n",
    "webpages_df = webpages_df[['text', 'label']]\n",
    "pdf_df = pdf_df[['text', 'label']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "81728d10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scraped Fair Loans (New)\n",
    "fair_webpaths = glob.glob(\"../data/raw/web_scrapes/fair_loans/webpages/*.txt\")\n",
    "fair_texts = [open(f,\"r\",encoding=\"utf-8\").read() for f in fair_webpaths]\n",
    "new_fair_df = pd.DataFrame({'text': fair_texts, 'label': 'non_predatory'})\n",
    "\n",
    "# Scraped Predatory Loans (New)\n",
    "pred_webpaths = glob.glob(\"../data/raw/web_scrapes/predatory_loans/webpages/*.txt\")\n",
    "pred_texts = [open(f,\"r\",encoding=\"utf-8\").read() for f in pred_webpaths]\n",
    "new_pred_df = pd.DataFrame({'text': pred_texts, 'label': 'predatory'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "2ee5d60a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Combine All\n",
    "full_df = pd.concat([\n",
    "    reddit_df,\n",
    "    loan_docs_df,\n",
    "    webpages_df,\n",
    "    pdf_df,\n",
    "    new_fair_df,\n",
    "    new_pred_df\n",
    "], axis=0).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "6d709d54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final balanced dataset\n",
    "full_df = full_df.dropna(subset=['text', 'label'])\n",
    "full_df = full_df[full_df['text'].str.strip() != \"\"].reset_index(drop=True)\n",
    "\n",
    "# Clean known keyword leakage\n",
    "KEYWORDS = [\n",
    "    \"credit union\", \"union\", \"payday\", \"loanmart\", \"loan mart\", \"lending\", \n",
    "    \"cashnet\", \"advance america\", \"quick cash\", \"title loan\", \"speedy cash\",\n",
    "    \"tribal loan\", \"easy finance\", \"short-term loan\", \"bad credit loan\",\n",
    "    \"instant cash\", \"get money\", \"fast loan\", \"borrow instantly\"\n",
    "]\n",
    "\n",
    "def clean_keywords(text):\n",
    "    for word in KEYWORDS:\n",
    "        text = text.lower().replace(word, \"\")\n",
    "    return text\n",
    "\n",
    "full_df['text_clean'] = full_df['text'].apply(clean_keywords)\n",
    "full_df = full_df.drop_duplicates(subset=[\"text_clean\"])\n",
    "\n",
    "# Balance the dataset across label\n",
    "min_size = full_df['label'].value_counts().min()\n",
    "full_df = full_df.groupby(\"label\").sample(n=min_size, random_state=42).reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "ef3a3c63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save cleaned full_df\n",
    "full_df.to_csv(\"../data/processed/full_dataset_final.csv\", index=False)\n",
    "full_df[['text', 'text_clean', 'label']].to_csv(\"../data/processed/full_dataset_with_cleaned.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58ae8200",
   "metadata": {},
   "source": [
    "## 3: ML Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7adc1a4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    full_df['text_clean'],\n",
    "    full_df['label'],\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=full_df['label']\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41a2290a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a TF-IDF vectorizer\n",
    "tfidf = TfidfVectorizer(\n",
    "    max_features=5000,\n",
    "    ngram_range=(1, 2), # Unigrams and bigrams\n",
    "    lowercase=True,\n",
    "    stop_words='english'\n",
    ")\n",
    "\n",
    "X_train_vec = tfidf.fit_transform(X_train)\n",
    "X_test_vec = tfidf.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65bdedcc",
   "metadata": {},
   "source": [
    "### Naive Bayes Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70d1d709",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Naive Bayes\n",
    "nb = MultinomialNB()\n",
    "nb.fit(X_train_vec, y_train)\n",
    "y_pred_nb = nb.predict(X_test_vec)\n",
    "\n",
    "# Evaluate\n",
    "print(\"=== Naive Bayes Classification Report ===\")\n",
    "print(classification_report(y_test, y_pred_nb))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68c2862d",
   "metadata": {},
   "outputs": [],
   "source": [
    "### function to plot confusion matrices for models\n",
    "from sklearn.metrics import confusion_matrix\n",
    "def plot_confusion(y_true, y_pred, title=\"Confusion Matrix\"):\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=[\"predatory\", \"non_predatory\"])\n",
    "    plt.figure(figsize=(6, 5))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=[\"predatory\", \"non_predatory\"], yticklabels=[\"predatory\", \"non_predatory\"])\n",
    "    plt.title(title)\n",
    "    plt.ylabel('Actual')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0e664eb",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edb9fb08",
   "metadata": {},
   "outputs": [],
   "source": [
    "## train log reg\n",
    "\n",
    "lr = LogisticRegression(max_iter=1000)\n",
    "lr.fit(X_train_vec, y_train)\n",
    "y_pred_lr = lr.predict(X_test_vec)\n",
    "\n",
    "print(\"Logistic Regression Classification Report:\")\n",
    "print(classification_report(y_test, y_pred_lr))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8acb79dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# confusion matrix for Logistic Regression\n",
    "plot_confusion(y_test, y_pred_lr, \"Logistic Regression Confusion Matrix\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69f6280a",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "814fb0a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Train Random forests\n",
    "\n",
    "rf = RandomForestClassifier()\n",
    "rf.fit(X_train_vec, y_train)\n",
    "y_pred_rf = rf.predict(X_test_vec)\n",
    "\n",
    "print(\"Random Forest Classification Report:\")\n",
    "print(classification_report(y_test, y_pred_rf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ed1f7b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix For Random Forest\n",
    "plot_confusion(y_test, y_pred_rf, \"Random Forest Confusion Matrix\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c24b5d3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "##light Model tuning for RF\n",
    "\n",
    "# Try a slightly smaller and slightly larger depth to see if better\n",
    "rf_model = RandomForestClassifier(\n",
    "    n_estimators=100,\n",
    "    max_depth=10,       # Try 10 instead of default (None)\n",
    "    min_samples_split=5,  # Avoid too much overfitting\n",
    "    random_state=42\n",
    ")\n",
    "rf_model.fit(X_train_vec, y_train)\n",
    "y_pred_rf_tuned = rf_model.predict(X_test_vec)\n",
    "\n",
    "print(\"Tuned Random Forest Classification Report:\")\n",
    "print(classification_report(y_test, y_pred_rf))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef356b08",
   "metadata": {},
   "source": [
    "### Support Vector Machine (SVM)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8adee39f",
   "metadata": {},
   "source": [
    "### Random Forest Tuning with GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e46d9367",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Set up gridseachcv for Random Forest\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "# Define the parameter grid\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200],\n",
    "    'max_depth': [None, 10, 20],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'max_features': ['sqrt', 'log2']\n",
    "}\n",
    "\n",
    "# Set up the Grid Search\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "grid_search = GridSearchCV(rf, param_grid, cv=3, n_jobs=-1, verbose=1)\n",
    "\n",
    "# Fit\n",
    "grid_search.fit(X_train_vec, y_train)\n",
    "\n",
    "# Best model\n",
    "best_rf_model = grid_search.best_estimator_\n",
    "\n",
    "# Predict\n",
    "y_pred_best_rf = best_rf_model.predict(X_test_vec)\n",
    "\n",
    "# See best parameters\n",
    "print(\"Best Random Forest params:\", grid_search.best_params_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a87d01f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "###Best Random Forest params: {'max_depth': None, 'max_features': 'log2', 'min_samples_leaf': 1, 'min_samples_split': 5, 'n_estimators': 200}\n",
    "\n",
    "rf_model = RandomForestClassifier(\n",
    "    n_estimators=200,\n",
    "    min_samples_split=5,\n",
    "    min_samples_leaf=1,\n",
    "    random_state=42,\n",
    "    max_features='log2'\n",
    ")\n",
    "rf_model.fit(X_train_vec, y_train)\n",
    "y_pred_rf = rf_model.predict(X_test_vec)\n",
    "print(\"Tuned Random Forest Classification Report:\")\n",
    "print(classification_report(y_test, y_pred_rf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a72d85c",
   "metadata": {},
   "outputs": [],
   "source": [
    "svm = LinearSVC(max_iter=5000)\n",
    "svm.fit(X_train_vec, y_train)\n",
    "y_pred_svm = svm.predict(X_test_vec)\n",
    "print(\"=== SVM classification report ===\")\n",
    "print(classification_report(y_test, y_pred_svm))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8389012",
   "metadata": {},
   "source": [
    "### Grid Search for SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "752435c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up a grid to search \n",
    "param_grid = {\n",
    "    'C': [0.1, 1, 10],\n",
    "    'kernel': ['linear', 'rbf'],\n",
    "    'gamma': ['scale', 'auto']\n",
    "}\n",
    "# Set up the Grid Search\n",
    "svm = SVC(random_state=42)\n",
    "grid_search = GridSearchCV(svm, param_grid, cv=3, n_jobs=-1, verbose=1)\n",
    "# Fit\n",
    "grid_search.fit(X_train_vec, y_train)\n",
    "# Best model    \n",
    "best_svm_model = grid_search.best_estimator_\n",
    "# Predict\n",
    "y_pred_best_svm = best_svm_model.predict(X_test_vec)\n",
    "# See best parameters\n",
    "print(\"Best SVM params:\", grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f2dd90d",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Best SVM params: {'C': 10, 'gamma': 'scale', 'kernel': 'linear'}\n",
    "##Grid Search results for SVM:\n",
    "\n",
    "svm_model = SVC(\n",
    "    C=10,\n",
    "    gamma='scale',\n",
    "    kernel='linear',\n",
    "    random_state=42\n",
    ")\n",
    "svm_model.fit(X_train_vec, y_train)\n",
    "y_pred_svm = svm_model.predict(X_test_vec)\n",
    "print(\"Tuned SVM Report:\")\n",
    "print(classification_report(y_test, y_pred_svm))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76527257",
   "metadata": {},
   "source": [
    "### Model Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dc2d071",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a helper function for multiclass\n",
    "def evaluate_model(name, y_true, y_pred, results=[]):\n",
    "    precision = precision_score(y_true, y_pred, average='weighted')\n",
    "    recall = recall_score(y_true, y_pred, average='weighted')\n",
    "    f1 = f1_score(y_true, y_pred, average='weighted')\n",
    "    results.append({\n",
    "        'Model': name,\n",
    "        'Precision': round(precision, 2),\n",
    "        'Recall': round(recall, 2),\n",
    "        'F1-Score': round(f1, 2)\n",
    "    })\n",
    "    return results\n",
    "\n",
    "# Initialize results list\n",
    "results = []\n",
    "\n",
    "# Evaluate each model\n",
    "results = evaluate_model(\"Logistic Regression\", y_test, y_pred_lr, results)\n",
    "results = evaluate_model(\"Naive Bayes\", y_test, y_pred_nb, results)\n",
    "results = evaluate_model(\"Random Forest\", y_test, y_pred_rf, results)\n",
    "results = evaluate_model(\"Random Forest (tuned)\", y_test, y_pred_rf_tuned, results)\n",
    "results = evaluate_model(\"Random Forest (grid search tuned)\", y_test, y_pred_best_rf, results)\n",
    "results = evaluate_model(\"SVM\", y_test, y_pred_svm, results)\n",
    "results = evaluate_model(\"SVM (grid search tuned)\", y_test, y_pred_best_svm, results)\n",
    "\n",
    "\n",
    "\n",
    "# Turn into DataFrame\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# Display\n",
    "results_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4404fbb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import dump\n",
    "dump(lr, \"../best_model.joblib\")\n",
    "dump(tfidf, \"../tfidf_vectorizer.joblib\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "897c2bcc",
   "metadata": {},
   "source": [
    "## BERT Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16c5cbc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokenization\n",
    "label_mapping = {'predatory': 1, 'non_predatory': 0}\n",
    "full_df = full_df[full_df['label'].isin(label_mapping)].reset_index(drop=True)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    full_df['text'],\n",
    "    full_df['label'],\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=full_df['label']\n",
    ")\n",
    "\n",
    "y_train_int = y_train.map(label_mapping)\n",
    "y_test_int = y_test.map(label_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19e72917",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = Dataset.from_dict({\n",
    "    'text': X_train.tolist(),\n",
    "    'label': y_train_int.tolist()\n",
    "})\n",
    "test_dataset = Dataset.from_dict({\n",
    "    'text': X_test.tolist(),\n",
    "    'label': y_test_int.tolist()\n",
    "})\n",
    "\n",
    "tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f43702d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(batch):\n",
    "    return tokenizer(batch['text'], padding=True, truncation=True)\n",
    "\n",
    "train_dataset = train_dataset.map(tokenize, batched=True)\n",
    "test_dataset = test_dataset.map(tokenize, batched=True)\n",
    "\n",
    "train_dataset.set_format('torch', columns=['input_ids','attention_mask','label'])\n",
    "test_dataset.set_format('torch', columns=['input_ids','attention_mask','label'])\n",
    "\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)\n",
    "model.config.problem_type = \"single_label_classification\"\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=4,\n",
    "    learning_rate=2e-5,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18434d57",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)\n",
    "model.config.problem_type = \"single_label_classification\"\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=4,\n",
    "    learning_rate=2e-5,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    "    # you can add: save_strategy='no' if you don't want checkpoints\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    ")\n",
    "# run training\n",
    "trainer.train()\n",
    "\n",
    "# still fine to call .predict afterwards…\n",
    "preds = trainer.predict(test_dataset)\n",
    "y_pred = torch.argmax(torch.tensor(preds.predictions), dim=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8bc8a30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict and Evaluate\n",
    "preds = trainer.predict(test_dataset)\n",
    "y_pred = torch.argmax(torch.tensor(preds.predictions), dim=1)\n",
    "\n",
    "print(\"=== BERT Classification Report ===\")\n",
    "print(classification_report(y_test_int, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5230d5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# - Fine-tune BERT with better hyperparameters"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
